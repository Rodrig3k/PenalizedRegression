{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized Linear Regression in Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "## Luis Pedro Coelho"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "<div style=\"padding: 8em\">\n",
      "Homepage: <a href=\"http://luispedro.org\">luispedro.org</a><br />\n",
      "Email: <a href=\"mailto:luis@luispedro.org\">luis@luispedro.org</a><br />\n",
      "<img src=\"twitter32.png\" style=\"margin: 0px\" /><a href=\"http://twitter.com/luispedrocoelho\">@luispedrocoelho</a><br />\n",
      "<img src=\"github32.png\" style=\"margin: 0px\"  /><a href=\"http://github.com/luispedro\">github/luispedro</a><br />\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Notes on the presentation\n",
      "\n",
      "This presentation is an Ipython Notebook, which you get at [github.com/luispedro/PenalizedRegression](https://github.com/luispedro/PenalizedRegression) or view using the [NBViewer](http://nbviewer.ipython.org/github/luispedro/PenalizedRegression/blob/master/PenalizedRegression.ipynb).\n",
      "\n",
      "\n",
      "## Dependencies\n",
      "\n",
      "To run this, you need\n",
      "\n",
      "- ipython, version 2\n",
      "- numpy\n",
      "- matplotlib (for the plots)\n",
      "- scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Preliminary imports\n",
      "\n",
      "We will import `numpy` using the `np` abbreviation and `matplplotlib.pyplot` using the `plt` abbreviation:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython.display import HTML, display"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We also need to perform some magic to get plots inline:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Regression\n",
      "\n",
      "Regression can be used generically to mean \"any type of learning from data.\"\n",
      "\n",
      "More commonly, though, it is used to mean *learn to predict a **numeric** output from variables*. As opposed to **classification** which learns to predict a categorical output.\n",
      "\n",
      "### Examples\n",
      "\n",
      "- predicting prices\n",
      "- predicting blood sugar levels\n",
      "- predicting product ratings (collaborative filtering)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Example: Boston House prices\n",
      "\n",
      "The goal is to predict house prices in Boston based on variables such as\n",
      "\n",
      "1. number of rooms\n",
      "2. crime rate in area\n",
      "3. pupil teacher ratio\n",
      "4. ..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Loading data\n",
      "\n",
      "The `boston` dataset comes built in with scikit-learn:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_boston\n",
      "boston = load_boston()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "You can use `print(boston.DESCR)` to see more information on the dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Split into testing & training\n",
      "\n",
      "For all our analyses, it will be important to have split training & testing data.\n",
      "\n",
      "Scikit-learn makes this easy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split \n",
      "train_data, test_data, train_target, test_target = \\\n",
      "        train_test_split(boston.data, boston.target, train_size=.8)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- `train_data` is our training data with corresponding target variable `train_target`.\n",
      "- `test_data` and `test_target` are the equivalent testing variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Having a peak at the data\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is always a good idea to peak around, get a feeling for the data.\n",
      "\n",
      "Extremely important to look out for anomalies (real data is rarely clean)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(train_data.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_=plt.hist(train_target, bins=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We can look at specific elements in the input data, such as the number of rooms:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RoomNr_Index = 5\n",
      "\n",
      "fig,ax = plt.subplots()\n",
      "ax.scatter(train_data[:,RoomNr_Index], train_target)\n",
      "ax.set_xlabel(\"Number of rooms\")\n",
      "ax.set_ylabel(\"House price\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Using IPython Notebook for exploration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.html.widgets import interact\n",
      "from scipy import stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use the same plotting code, just generalized to take an `index` argument and wrapped with `interact`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@interact(index=(0, train_data.shape[1]))\n",
      "def plot_scatter(index):\n",
      "    fig,ax = plt.subplots()\n",
      "    x, y = train_data[:,index], train_target\n",
      "    ax.scatter(x, y)\n",
      "    ax.set_xlabel(boston.feature_names[index])\n",
      "    ax.set_ylabel(\"House price\")\n",
      "    print(\"Correlation: {0[0]:.1} (p-value: {0[1]:.1})\".format(stats.pearsonr(x, y)))\n",
      "    return fig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The `interact` features are the killer app for Ipython notebook."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Can we build a model, then?\n",
      "\n",
      "We have now poked around the data, can we build a predictive model?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Ordinary Least Squares Linear Regression\n",
      "\n",
      "Let $x$ be our inputs and $y$ our outputs.\n",
      "\n",
      "### Simple 1-D version\n",
      "\n",
      "Our *generative* model of the data is simply that given an input $x$, we obtain $y$ by\n",
      "\n",
      "1. multiply by $\\beta$,\n",
      "2. adding a constant ($c$)\n",
      "3. adding some noise ($\\epsilon$)\n",
      "\n",
      "$$y = \\beta x + c + \\epsilon$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### Multidimensional linear regression\n",
      "\n",
      "Instead of a single input variable, we can have multiple inputs, $x_1$, $x_2$, ... $x_n$:\n",
      "\n",
      "$$y = \\sum_j \\beta_j x_j + c + \\epsilon$$\n",
      "\n",
      "We can write the same using vector notation as:\n",
      "\n",
      "$$y = {\\boldsymbol \\beta}^T {\\bf x} + c + \\epsilon$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Least squares regression\n",
      "\n",
      "- In real life, we have observations and want to fit a model.\n",
      "- When we have more observations than datapoints, there is very little chance that a model will fit perfectly.\n",
      "- Therefore, we **minimise the fitting error**.\n",
      "\n",
      "$$y_i = {\\boldsymbol \\beta}^T {\\bf x}_i + c + \\epsilon_i$$\n",
      "\n",
      "We can measure the error as the sum of squared errors:\n",
      "\n",
      "$$ E = \\sum_i \\epsilon^2_i $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The criterion is thus\n",
      "\n",
      "$$ \\beta^{*} = \\arg\\min_{\\boldsymbol \\beta} \\sum_i ({\\boldsymbol \\beta}^T {\\bf x}_i + c - y_i)^2 $$\n",
      "\n",
      "which we can write in short form as:\n",
      "\n",
      "$$ \\beta^{*} = \\arg\\min_{\\boldsymbol \\beta} \\sum_i \\epsilon^2_i $$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### A few notes on the square function\n",
      "\n",
      "\n",
      "Pretty obvious but it'll come in handy later too:\n",
      "\n",
      "- larger error are penalized much more heavily than small error\n",
      "- doubling the error quadruples the penalty"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "E = np.linspace(0, 2, 100)\n",
      "squared_error_figure, ax = plt.subplots(figsize=(6,6))\n",
      "ax.plot(E, E**2)\n",
      "ax.plot([0,1], [1,1], 'k-')\n",
      "ax.plot([1,1], [0,1], 'k-')\n",
      "ax.set_xlim(0,4.)\n",
      "ax.set_xlabel('$E$')\n",
      "ax.set_ylabel('$E^2$')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "display(squared_error_figure)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "#### Historical note\n",
      "\n",
      "This method of least squares was first developed to estimate the motion of cellular bodies by Gauss who was officially an astronomer. He also proved that least squares is optimal under *Gaussian* noise.\n",
      "\n",
      "![Gauss](http://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Carl_Friedrich_Gauss.jpg/330px-Carl_Friedrich_Gauss.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Today we would call this the *criterion of least squares* as method seems to imply that we have an algorithm to solve the problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Using OLS on the boston data with scikit-learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "linreg = linear_model.LinearRegression()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1D version"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_=linreg.fit(train_data[:,RoomNr_Index:RoomNr_Index+1], train_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig,ax = plt.subplots()\n",
      "ax.scatter(train_data[:,RoomNr_Index], train_target)\n",
      "ax.plot([4, 9], linreg.predict([[4],[9]]), 'k-')\n",
      "ax.set_xlabel(\"Number of rooms\")\n",
      "_=ax.set_ylabel(\"House price\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Multi-dimensional version\n",
      "\n",
      "We can use the same `linreg.fit` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linreg.fit(train_data, train_target)\n",
      "prediction = linreg.predict(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Evaluating the results\n",
      "\n",
      "### Mean squared error\n",
      "\n",
      "This is a very natural way to measure the error:\n",
      "\n",
      "$$MSE = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
      "\n",
      "where $y_i$ is the actual value for example $i$, and $\\hat{y}_i$ is the prediction for the same example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "mse = metrics.mean_squared_error(test_target, linreg.predict(test_data)) \n",
      "print(\"MSE is {}\".format(mse))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Problem**: it is not trivial to interpret the value obtained."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Root mean squared error\n",
      "\n",
      "$$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}{N} (y_i - \\hat{y}_i)^2}$$\n",
      "\n",
      "1. Same units as original target variable\n",
      "2. If errors really are normal, then just double this value for an estimate of 95% confidence intervals."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rmse = np.sqrt(mse) \n",
      "print(\"RMSE is {}\".format(rmse))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Coefficient of determination\n",
      "\n",
      "Sometimes, there is another value that is very useful, the *coefficient of determination*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "*Idea*: compare the RSME with a baseline."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Consider the null predictor, which ignores the input and always returns a contant:\n",
      "\n",
      "    def predict(features):\n",
      "        return constant\n",
      "\n",
      "The *best constant is the mean*:\n",
      "\n",
      "    def predict(features):\n",
      "        return mean_of_training_target"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Now, compute the ratio of error this predictor with the error of your predictor."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "$$\\text{COD} = 1 - \\frac{ \\sum_i (y_i -\\hat{y}_i)^2}{ \\sum_i  (y_i - \\bar{y})^2}\n",
      "    \\approx 1 - \\frac{\\text{MSE}}{\\text{VAR}(y)}$$\n",
      "\n",
      "\n",
      "Where $\\bar{y}_i$ is the *average value of $y$, i.e.:\n",
      "\n",
      "$$\\bar{y} = \\frac{\\sum_i y_i}{N}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Evaluating COD in Boston dataset\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cod = metrics.r2_score(test_target, linreg.predict(test_data))\n",
      "print(\"COD is {}\".format(cod))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This measure is also called $R^2$, which is confusing because it is not the square of anything.\n",
      "\n",
      "It is also called $Q^2$ sometimes, which is also confusing.\n",
      "\n",
      "This measure is the default measure for regression in scikit-learn and we can just use the `score` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(linreg.score(test_data, test_target))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Training vs. Testing Data\n",
      "\n",
      "Naturally, the results on the training data are better than those obtained on the testing data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linreg.fit(train_data, train_target)\n",
      "r2_train = metrics.r2_score(train_target, linreg.predict(train_data))\n",
      "r2_test = metrics.r2_score(test_target, linreg.predict(test_data))\n",
      "\n",
      "print(\"R2 on training: {:.1%}\".format(r2_train))\n",
      "print(\"R2 on testing: {:.1%}\".format(r2_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Visualizing the fit\n",
      "\n",
      "We cannot easily see the fit in high dimensions, but we can plot *measured vs prediction*:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linreg.fit(train_data, train_target)\n",
      "fig,ax = plt.subplots()\n",
      "ax.scatter(test_target, linreg.predict(test_data))\n",
      "ax.plot([0,50], [0,50], 'k-')\n",
      "ax.set_xlabel('Target (test)')\n",
      "ax.set_ylabel('Predicted (test)')\n",
      "pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Penalized (or Regularized) regression\n",
      "\n",
      "The criterion for least squared regression was:\n",
      "\n",
      "$$\\beta = \\arg\\min \\sum_j \\epsilon^2_j$$\n",
      "\n",
      "That is, *we minimize the fit on the training data*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The general expresssion for penalized regression is\n",
      "\n",
      "$$\\beta = \\arg\\min \\frac{1}{2N} \\sum_j \\epsilon^2_j + \\alpha R({\\boldsymbol \\beta}),$$\n",
      "\n",
      "where $R$ is the penalty (regularization) term and $\\alpha$ is a positive weight.\n",
      "\n",
      "That is, we minimize the **sum of the fit plus a regularization term**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "This is an instance of the [Bias-Variance Tradeoff](http://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### What about the intercept?\n",
      "\n",
      "- The intercept is typically not penalized.\n",
      "- One can simply pre-center the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### L1 and L2 penalties\n",
      "\n",
      "$L_1$ penalty means that we use the *sum of **absolute values***:\n",
      "\n",
      "$$P_1({\\boldsymbol \\beta}) = \\sum_j |\\beta_j|$$\n",
      "\n",
      "$L_2$ penalty means that we use the *sum of **squares***:\n",
      "\n",
      "$$P_2({\\boldsymbol \\beta}) = \\sum_j \\beta_j^2$$\n",
      "\n",
      "(The names come from the generic concept of an $L_p$ norm, see [Wikipedia](http://en.wikipedia.org/wiki/Lp_space) for details)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### L1 penalty: the lasso\n",
      "\n",
      "Using the L1 norm, leads us to the *Lasso*!\n",
      "\n",
      "Lasso stands for *least absolute shrinkage and selection operator*, but nobody uses that long name."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### In scikit-learn:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso = linear_model.Lasso()\n",
      "lasso.fit(train_data, train_target)\n",
      "pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setting the $\\alpha$ parameter:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso = linear_model.Lasso(alpha=0.1)\n",
      "lasso.fit(train_data, train_target)\n",
      "pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(We will discuss how to set the $\\alpha$ parameter in a smarter later in the talk)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Normalizing the inputs\n",
      "\n",
      "Some features span much higher values. Thus, when our penalty adds all the weights together, it implicitly weighs the low varying features more.\n",
      "\n",
      "With OLS, this does not matter, but it is important to normalize the inputs when using Lasso."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig,ax = plt.subplots()\n",
      "ax.plot(train_data.ptp(0))\n",
      "ax.set_xticklabels(boston.feature_names)\n",
      "ax.set_xticks(np.arange(train_data.shape[1]))\n",
      "ax.set_ylabel('Point to point (max-min)')\n",
      "pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Normalization ensures that all the inputs are on the same scale."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso = linear_model.Lasso(normalize=True, alpha=.1)\n",
      "lasso.fit(train_data, train_target)\n",
      "pass"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Evaluating Lasso vs. OLS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linreg.fit(train_data, train_target)\n",
      "r2_ols_train = linreg.score(train_data, train_target)\n",
      "r2_ols = linreg.score(test_data, test_target)\n",
      "\n",
      "lasso.fit(train_data, train_target)\n",
      "r2_lasso_train = lasso.score(train_data, train_target)\n",
      "r2_lasso = lasso.score(test_data, test_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = \"\"\"\\\n",
      "      | TRAINING | TESTING\n",
      "------+----------+---------\n",
      "OLS   | {:.2%}   | {:.2%}\n",
      "------+----------+---------\n",
      "Lasso | {:.2%}   | {:.2%}\n",
      "---------------------------\n",
      "\"\"\".format(r2_ols_train, r2_ols, r2_lasso_train, r2_lasso)\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a bit, we will see another dataset, where Lasso shines brighter."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### L2 penalty: Ridge regression\n",
      "\n",
      "Using the L2 norm, leads us to *Ridge* regression (a.k.a. Tikhonov regularization).\n",
      "\n",
      "Unlike Lasso, which was invented only recently (1996), Ridge goes back a few decades."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ridge = linear_model.Ridge(normalize=True, alpha=.1)\n",
      "\n",
      "ridge.fit(train_data, train_target)\n",
      "r2_ridge_train = ridge.score(train_data, train_target)\n",
      "r2_ridge = ridge.score(test_data, test_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = (\"\"\"\\\n",
      "      | TRAINING | TESTING\n",
      "------+----------+---------\n",
      "OLS   | {:.2%}   | {:.2%}\n",
      "------+----------+---------\n",
      "Lasso | {:.2%}   | {:.2%}\n",
      "------+----------+---------\n",
      "Ridge | {:.2%}   | {:.2%}\n",
      "---------------------------\n",
      "\"\"\".format(r2_ols_train, r2_ols,\n",
      "           r2_lasso_train, r2_lasso,\n",
      "           r2_ridge_train, r2_ridge)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "print(results)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Lasso returns a sparse solution\n",
      "\n",
      "This means that **some coefficients are exactly zero**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(lasso.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use Lasso to select important features!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "## Higher penalties (higher $\\alpha$) means more sparsity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso.alpha = .2\n",
      "lasso.fit(train_data, train_target)\n",
      "print(lasso.coef_)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Too high just returns zeros everywhere:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso.alpha = 1.\n",
      "lasso.fit(train_data, train_target)\n",
      "print(lasso.coef_)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Looking at the Lasso Path\n",
      "\n",
      "This is a way to visualize what happens when we increase/decrease regularization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alphas = np.linspace(.01, 1000., 1000)\n",
      "alphas, coefs, _= lasso.path(train_data, train_target, alphas=alphas)\n",
      "\n",
      "fig,ax = plt.subplots()\n",
      "ax.plot(alphas, coefs.T)\n",
      "ax.set_xscale('log')\n",
      "ax.set_xlim(alphas.max(), alphas.min())\n",
      "ax.set_xlabel(r'$\\alpha$')\n",
      "ax.set_ylabel('Coefficient value')\n",
      "pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Another look at the path\n",
      "\n",
      "Instead of the values, we can also just count how many are non zero:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig,ax = plt.subplots()\n",
      "ax.plot(alphas, np.sum(coefs != 0.0, axis=0)) \n",
      "ax.set_xscale('log')\n",
      "ax.set_xlim(alphas.max(), alphas.min())\n",
      "ax.set_ylim(-.1, 13.1)\n",
      "ax.set_xlabel(r'$\\alpha$')\n",
      "ax.set_ylabel('Nr of non-zero coefficients')\n",
      "pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Ridge solutions are not sparse\n",
      "\n",
      "Unlike the Lasso, ridge does not set any coefficients to zero!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ridge.fit(train_data, train_target)\n",
      "print(ridge.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Comparing OLS, Lasso, and Ridge"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso.alpha = 0.1\n",
      "lasso.fit(train_data, train_target)\n",
      "ridge.alpha = 0.1\n",
      "ridge.fit(train_data, train_target)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig,ax = plt.subplots()\n",
      "ax.plot(linreg.coef_, label='OLS')\n",
      "ax.plot(lasso.coef_, label=r'Lasso ($\\alpha$=0.1)')\n",
      "ax.plot(ridge.coef_, label=r'Ridge ($\\alpha$=0.1)')\n",
      "ax.set_xticklabels(boston.feature_names)\n",
      "ax.set_xticks(np.arange(train_data.shape[1]))\n",
      "ax.legend(loc='best')\n",
      "pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- OLS is always larger than either Lasso or Ridge\n",
      "- Lasso sets some weights to zero; Ridge does not."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Why does the Lasso return a sparse solution while Ridge does not?\n",
      "\n",
      "#### Argument 1: algorithmic argument\n",
      "\n",
      "Consider the following pseudo-algorithm:\n",
      "\n",
      "1. Start with ${\\boldsymbol \\beta} = 0$.\n",
      "2. Find the best index to increment $j$ and spend a bit of your budget to move $\\beta_j$ in the right direction.\n",
      "3. If this is better than before, then **goto 2**; else, stop.\n",
      "\n",
      "Because Ridge uses squared penalties, $\\beta$s that are still stuck at zero are very cheap in step 2, so we might choose them even if they are not very good.\n",
      "\n",
      "With Lasso, however, they always cost the same. Therefore, the algorithm will typically keep choosing the same index repeatedtly.\n",
      "\n",
      "*Note*: An improved version of this idea forms the basis of the famous [Least-Angle Regression](http://en.wikipedia.org/wiki/Least-angle_regression) method to solve the Lasso problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "#### Argument 2: geometric argument\n",
      "\n",
      "- Consider Lasso (left) vs Ridge (right)\n",
      "\n",
      "![Figure 2](ridge-vs-lasso.png)\n",
      "\n",
      "- The filled areas are those were the penalty is below a certain limit, while the ellipsis show the isocontours.\n",
      "- With Lasso, they intersect at edges, with Ridge, in intermediate values.\n",
      "\n",
      "(This is Figure 2 from the paper *Regression shrinkage and selection via the lasso*, by R. Tibshirani [[JSTOR Version](http://www.jstor.org/stable/2346178)].)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Combining L1 and L2 penalties: elastic nets\n",
      "\n",
      "We can combine the two types of penalty to obtain *elastic nets*:\n",
      "\n",
      "$$P({\\boldsymbol \\beta}) = \\alpha_1 P_1({\\boldsymbol \\beta}) + \\alpha_2 P_2({\\boldsymbol \\beta}) = \\alpha_1 \\sum_j |\\beta_j| + \\alpha_2 \\sum_j \\beta_j^2$$\n",
      "\n",
      "We now have two penalties, with different weights, $\\alpha_1$ and $\\alpha_2$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "en = linear_model.ElasticNet(normalize=True)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Instead of $\\alpha_1$, and $\\alpha_2$, in scikit-learn one specifies two parameters, $\\alpha$ and $\\rho$ (a.k.a. `l1_ratio`)\n",
      "\n",
      "$$\\alpha_1 = \\rho \\alpha$$\n",
      "\n",
      "$$\\alpha_2 = \\frac{1}{2} (1-\\rho) \\alpha$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "en = linear_model.ElasticNet(normalize=True, alpha=0.1, l1_ratio=.5)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This way, `alpha` defines *amount of penalty* and `l1_ratio` how the penalty is split between L1 and L2."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## An Elastic net can interpolate between Lasso and Ridge"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso = linear_model.ElasticNet(normalize=True, alpha=0.1, l1_ratio=1.) \n",
      "ridge = linear_model.ElasticNet(normalize=True, alpha=0.1, l1_ratio=0.)\n",
      "half_way =linear_model.ElasticNet(normalize=True, alpha=0.1, l1_ratio=.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One issue with pure Lasso is that it is greedy and unstable\n",
      "\n",
      "- Between very similar features, it will choose the one that is best\n",
      "- Even if the differences are very slight\n",
      "- An elastic net with a small L2 penalty will make the learner \"split the difference\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "almost_lasso = linear_model.ElasticNet(normalize=True, alpha=0.1, l1_ratio=.95)\n",
      "almost_lasso.fit(train_data, train_target)\n",
      "lasso.fit(train_data, train_target)\n",
      "\n",
      "print(almost_lasso.coef_)\n",
      "print(lasso.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Tackling a large problem\n",
      "\n",
      "The boston dataset is small, so it does not fully do justice to the power of penalized regression.\n",
      "\n",
      "## The 10-K Corpus\n",
      "\n",
      "Quoting from the [online page](http://www.ark.cs.cmu.edu/10K/) for this dataset:\n",
      "\n",
      "> The corpus contains 10-K reports from many US companies during years 1996-2006, as well as measured volatility of stock returns for the twelve-month periods preceding and following each report\n",
      "\n",
      "This was generated and used in this paper:\n",
      "\n",
      "> Predicting Risk from Financial Reports with Regression\n",
      "Shimon Kogan, Dimitry Levin, Bryan R. Routledge, Jacob S. Sagi, and Noah A. Smith\n",
      "NAACL-HLT 2009, Boulder, CO, May\u2013June 2009\n",
      "[Online version of paper](http://www.cs.cmu.edu/~nasmith/papers/kogan+levin+routledge+sagi+smith.naacl09.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "If you are reading this at home, you need to download the E2006 dataset. You can use the following code block (unfortunately, this is Python 2 only code)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "source_location = 'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/E2006.train.bz2'\n",
      "\n",
      "from os import path\n",
      "if not path.exists('E2006.train.bz2'):\n",
      "    import urllib\n",
      "    urllib.urlretrieve(source_location, 'E2600.train.bz2')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Loading and Splitting the Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_svmlight_file\n",
      "E2006_data, E2006_target = load_svmlight_file('E2006.train.bz2')\n",
      "print(E2006_data.shape)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have 16k examples and 150k features!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(type(E2006_data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataset is staored in memory as a sparse matrix as most entries are zero.\n",
      "\n",
      "## Split into train/test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "E2_train_data, E2_test_data, E2_train_target, E2_test_target = \\\n",
      "        train_test_split(E2006_data, E2006_target, train_size=.8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Trying OLS on E2006 dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linreg.fit(E2_train_data, E2_train_target)\n",
      "r2_ols_train = linreg.score(E2_train_data, E2_train_target)\n",
      "r2_ols = linreg.score(E2_test_data, E2_test_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Note that we just used the same code as before even though we are using a sparse matrix!\n",
      "- Underneath the hood, scikit-learn selects the right implementation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"R2 Training (OLS): {:.1%}\".format(r2_ols_train))\n",
      "print(\"R2 Testing (OLS): {:.1%}\".format(r2_ols))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "(Note that you may get different values as the train/test split was random)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Ordinary Least Squares fails when \"P > N\"\n",
      "\n",
      "- $P$ is the number of dimensions\n",
      "- $N$ is the number of examples\n",
      "- when $P > N$, we have more dimensions than examples\n",
      "- mathematically, it \"always\" fits training data perfectly!, but generalizes poorly"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso = linear_model.Lasso(normalize=True, alpha=.001)\n",
      "lasso.fit(E2_train_data, E2_train_target)\n",
      "\n",
      "r2_lasso_train = lasso.score(E2_train_data, E2_train_target)\n",
      "r2_lasso = lasso.score(E2_test_data, E2_test_target)\n",
      "ridge = linear_model.Ridge(normalize=True, alpha=.1)\n",
      "ridge.fit(E2_train_data, E2_train_target)\n",
      "\n",
      "r2_ridge_train = ridge.score(E2_train_data, E2_train_target)\n",
      "r2_ridge = ridge.score(E2_test_data, E2_test_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results_p_gt_n = \"\"\"\\\n",
      "      | TRAINING | TESTING\n",
      "------+----------+---------\n",
      "OLS   | {:.2%}   | {:.2%}\n",
      "------+----------+---------\n",
      "Lasso | {:.2%}   | {:.2%}\n",
      "------+----------+---------\n",
      "Ridge | {:.2%}   | {:.2%}\n",
      "---------------------------\n",
      "\"\"\".format(r2_ols_train, r2_ols,\n",
      "           r2_lasso_train, r2_lasso,\n",
      "           r2_ridge_train, r2_ridge)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(results_p_gt_n)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Fitting hyperparameters properly\n",
      "\n",
      "Those $\\alpha$ values I set, worked well for the examples, but why did I use them and not others?\n",
      "\n",
      "So far, we have been **cheating** and should be sent to machine learning re-education camp.\n",
      "\n",
      "- We need to set parameters (I've heard it argued that *setting hyperparameters is the last big open problem in statistics*)\n",
      "- So far, we hand waived this problem away\n",
      "- I gave you parameters that worked because I cheated!\n",
      "- Remember that parameters depend on scale of inputs (sklearn normalization scales variables, but not target)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Generic parameter setting solution\n",
      "\n",
      "Validation dataset or cross-validation!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- These technique work well for this problem, because we use *warm starts*.\n",
      "- We already took advantage of this fact, when using the `path` method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "enCV = linear_model.ElasticNetCV(normalize=True,\n",
      "                                 l1_ratio=[.1, .5, .7, .9, .95, .99, 1],\n",
      "                                 )"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Use multiple processors**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "enCV = linear_model.ElasticNetCV(normalize=True,\n",
      "                                 l1_ratio=[.1, .5, .7, .9, .95, .99, 1],\n",
      "                                 n_jobs=-1,\n",
      "                                 )"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Let me repeat that"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "enCV = linear_model.ElasticNetCV(normalize=True,\n",
      "                                 l1_ratio=[.1, .5, .7, .9, .95, .99, 1],\n",
      "                                 n_jobs=-1,\n",
      "                                 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This can be your **default object for regression**\n",
      "\n",
      "- Generally applicable\n",
      "- Takes advantage of multiple CPUs\n",
      "- Automatically sets hyperparameters\n",
      "- Explores different models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Final solution for E2006 dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Note: only on very up to date versions of scikit-learn (github version right now) does this work well on my computer.\n",
      "\n",
      "Older versions take too much memory and therefore are better tackled with a single CPU:\n",
      "\n",
      "     enCV.n_jobs = 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "enCV.fit(E2_train_data, E2_train_target)\n",
      "r2_enCV_train = enCV.score(E2_train_data, E2_train_target)\n",
      "r2_enCV = enCV.score(E2_test_data, E2_test_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This may take a while..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result_en_vc = \"\"\"\\\n",
      "      | TRAINING | TESTING\n",
      "------+----------+---------\n",
      "OLS   | {:.2%}   | {:.2%}\n",
      "------+----------+---------\n",
      "Lasso | {:.2%}   | {:.2%}\n",
      "------+----------+---------\n",
      "Ridge | {:.2%}   | {:.2%}\n",
      "------+----------+---------\n",
      "EN-CV | {:.2%}   | {:.2%}\n",
      "---------------------------\n",
      "\"\"\".format(r2_ols_train, r2_ols,\n",
      "           r2_lasso_train, r2_lasso,\n",
      "           r2_ridge_train, r2_ridge,\n",
      "           r2_enCV_train,  r2_enCV))\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(result_en_cv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Loose Ends"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are some parameters which can be interesting, which we did not look at:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(enCV)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- `alphas` lets you specify the $\\alpha$ values to test (similar to `l1_ratio`). It is fine to leave this unspecified.\n",
      "- `fit_intercept` can be set to `False` to avoid setting an intercept.\n",
      "- `n_jobs` sets the number of job (-1 means all CPUs)\n",
      "- `positive` can be used to force all coefficients to be positive.\n",
      "\n",
      "All the other ones are algorithm-internal and useful mainly if you are having problems (try updating scikit-learn first, though, newer versions have better implementations for large problems)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Conclusions\n",
      "\n",
      "- Linear regression is a solid trick\n",
      "- Lasso gives sparse solutions, Ridge does not.\n",
      "- Lasso can be used as a feature selection method.\n",
      "- Elastic nets combines both penalties\n",
      "- Use `ElasticNetCV` with a wide range of inputs as your go to regression\n",
      "- [scikit-learn](http://scikit-learn.org/) is great"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Thank you for your time"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This presentation was based on Chapter 7 (*Regression*) of [Building Machine Learning Systems with Python](http://shop.oreilly.com/product/9781782161400.do)\n",
      "\n",
      "\n",
      "Find me at [luispedro.org](http://luispedro.org) or on twitter at [@luispedrocoelho](https://twitter.com/luispedrocoelho)\n",
      "\n",
      "\n",
      "This whole presentation can be obtained at [github.com/luispedro/PenalizedRegression](https://github.com/luispedro/PenalizedRegression) or viewed using the [NBViewer](http://nbviewer.ipython.org/github/luispedro/PenalizedRegression/blob/master/PenalizedRegression.ipynb)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}