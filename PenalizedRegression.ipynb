{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized Linear Regression in Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Luis Pedro Coelho"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "<a href=\"http://luispedro.org\">luispedro.org</a><br />\n",
      "<img src=\"twitter32.png\" /><a href=\"http://twitter.com/luispedrocoelho\">@luispedrocoelho</a><br />\n",
      "<img src=\"github32.png\" /><a href=\"http://github.com/luispedro\">repositories on github</a><br />\n",
      "<a href=\"http://bit.ly/MLPythonBook\">Building Machine Learning Systems with Python: Willi Richert, Luis Pedro Coelho</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Notes on the presentation\n",
      "\n",
      "This presentation is an Ipython Notebook, which you can run at:\n",
      "\n",
      "LINK for presentation\n",
      "\n",
      "## Dependencies\n",
      "\n",
      "To run this, you need\n",
      "\n",
      "- ipython, version 2\n",
      "- numpy\n",
      "- matplotlib (for the plots)\n",
      "- scikit-learn\n",
      "- pandas\n",
      "\n",
      "Pandas is just a convenience"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Preliminary imports\n",
      "\n",
      "We will import `numpy` using the `np` abbreviation and `matplplotlib.pyplot` using the `plt` abbreviation:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython.display import HTML, display"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We also need to perform some magic to get plots inline:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Regression\n",
      "\n",
      "Regression can be used generically to mean \"any type of learning from data.\"\n",
      "\n",
      "More commonly, though, it is used to mean *learn to predict a **numeric** output from variables*. As opposed to **classification** which learns to predict a categorical output.\n",
      "\n",
      "### Examples\n",
      "\n",
      "- predicting prices\n",
      "- predicting blood sugar levels\n",
      "- predicting product ratings (collaborative filtering)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Example: Boston House prices\n",
      "\n",
      "The goal is to predict house prices in Boston based on variables such as\n",
      "\n",
      "1. number of rooms\n",
      "2. crime rate in area\n",
      "3. pupil teacher ratio\n",
      "4. ...\n",
      "\n",
      "\n",
      "### Loading data\n",
      "\n",
      "The `boston` dataset comes built in with scikit-learn:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_boston\n",
      "boston = load_boston()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can use `print(boston.DESCR)` to see more information on the dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Split into testing & training\n",
      "\n",
      "For all our analyses, it will be important to have split training & testing data.\n",
      "\n",
      "Scikit-learn makes this easy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split \n",
      "train_data, test_data, train_target, test_target = \\\n",
      "        train_test_split(boston.data, boston.target, train_size=.8)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- `train_data` is our training data with corresponding target variable `train_target`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Having a peak at the data\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is always a good idea to peak around, get a feeling for the data.\n",
      "\n",
      "Extremely important to look out for anomalies (real data is rarely clean)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(train_data.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RoomNr_Index = 5\n",
      "\n",
      "fig,ax = plt.subplots()\n",
      "ax.scatter(train_data[:,RoomNr_Index], train_target)\n",
      "ax.set_xlabel(\"Number of rooms\")\n",
      "ax.set_ylabel(\"House price\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Ordinary Least Squares Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "linreg = linear_model.LinearRegression()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linreg.fit(train_data, train_target)\n",
      "fig,ax = plt.subplots()\n",
      "ax.scatter(test_target, linreg.predict(test_data))\n",
      "ax.set_xlabel('Target (test)')\n",
      "ax.set_ylabel('Predicted (test)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Ordinary Least Squares Linear Regression\n",
      "\n",
      "Let $x$ be our inputs and $y$ our outputs.\n",
      "\n",
      "### Simple 1-D version\n",
      "\n",
      "Our *generative* model of the data is simply that given an input $x$, we obtain $y$ by\n",
      "\n",
      "1. multiply by $\\alpha$,\n",
      "2. adding a constant ($c$)\n",
      "3. adding some noise ($\\epsilon$)\n",
      "\n",
      "$$y = \\alpha x + c + \\epsilon$$\n",
      "\n",
      "### Multidimensional linear regression\n",
      "\n",
      "Instead of a single input variable, we can have multiple inputs, $x_1$, $x_2$, ... $x_n$:\n",
      "\n",
      "$$y = \\sum_j \\alpha_j x_j + c + \\epsilon$$\n",
      "\n",
      "We can write the same using vector notation as:\n",
      "\n",
      "$$y = {\\boldsymbol \\alpha}^T {\\bf x} + c + \\epsilon$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Least squares regression\n",
      "\n",
      "- In real life, we have observations and want to fit a model.\n",
      "- When we have more observations than datapoints, there is very little chance that a model will fit perfectly.\n",
      "- Therefore, we **minimise the fitting error**.\n",
      "\n",
      "$$y_i = {\\boldsymbol \\alpha}^T {\\bf x}_i + c + \\epsilon_i$$\n",
      "\n",
      "We can measure the error as the sum of squared errors:\n",
      "\n",
      "$$ E = \\sum_i \\epsilon^2_i $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Historical note\n",
      "\n",
      "This method of least squares was first developed to estimate the motion of cellular bodies by Gauss who was officially an astronomer. He also proved that least squares is optimal under *Gaussian* noise.\n",
      "\n",
      "![Gauss](http://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Carl_Friedrich_Gauss.jpg/330px-Carl_Friedrich_Gauss.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### A few notes on the square function\n",
      "\n",
      "\n",
      "Pretty obvious but it'll come in handy later too:\n",
      "\n",
      "- larger error are penalized much more heavily than small error\n",
      "- doubling the error quadruples the penalty"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "E = np.linspace(0, 2, 100)\n",
      "squared_error_figure, ax = plt.subplots(figsize=(6,6))\n",
      "ax.plot(E, E**2)\n",
      "ax.plot([0,1], [1,1], 'k-')\n",
      "ax.plot([1,1], [0,1], 'k-')\n",
      "ax.set_xlim(0,4.)\n",
      "ax.set_xlabel('$E$')\n",
      "ax.set_ylabel('$E^2$')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "display(squared_error_figure)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Penalized (or Regularized) regression\n",
      "\n",
      "The criterion for least squared regression was:\n",
      "\n",
      "$$\\beta = \\arg\\min \\sum_j \\epsilon^2_j$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The general expresssion for penalized regression is\n",
      "\n",
      "$$\\beta = \\arg\\min \\sum_j \\epsilon^2_j + \\alpha R(\\beta),$$\n",
      "\n",
      "where $R$ is the penalty (regularization) term and $\\alpha$ is a positive weight."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### What about the intercept?\n",
      "\n",
      "- The intercept is typically not penalized.\n",
      "- One can simply pre-center the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### L1 and L2 penalties\n",
      "\n",
      "$L_1$ penalty means that we use the *sum of **absolute values***:\n",
      "\n",
      "$$P_1({\\boldsymbol \\beta}) = \\sum_j |\\beta_j|$$\n",
      "\n",
      "$L_2$ penalty means that we use the *sum of **squares***:\n",
      "\n",
      "$$P_2({\\boldsymbol \\beta}) = \\sum_j \\beta_j^2$$\n",
      "\n",
      "(The names come from the generic concept of an $L_p$ norm, see [Wikipedia](http://en.wikipedia.org/wiki/Lp_space) for details)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### L1 penalty: the lasso\n",
      "\n",
      "Using the L1 norm, leads us to the *Lasso*!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### L2 penalty: Ridge regression\n",
      "\n",
      "Using the L2 norm, leads us to *Ridge* regression (a.k.a. Tikhonov regularization).\n",
      "\n",
      "Unlike Lasso, which was invented only recently (1996), Ridge goes back a few decades."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Why does the Lasso return a sparse solution while Ridge does not?\n",
      "\n",
      "#### Argument 1: algorithmic argument\n",
      "\n",
      "Consider the following pseudo-algorithm:\n",
      "\n",
      "1. Start with ${\\boldsymbol \\beta} = 0$.\n",
      "2. Find the best index to increment $j$ and spend a bit of your budget to move $\\beta_j$ in the right direction.\n",
      "3. If this is better than before, then **goto 2**; else, stop.\n",
      "\n",
      "Because Ridge uses squared penalties, $\\beta$s that are still stuck at zero are very cheap in step 2, so we might choose them even if they are not very good.\n",
      "\n",
      "With Lasso, however, they always cost the same. Therefore, the algorithm will typically keep choosing the same index repeatedtly.\n",
      "\n",
      "*Note*: An improved version of this idea forms the basis of the famous [Least-Angle Regression](http://en.wikipedia.org/wiki/Least-angle_regression) method to solve the Lasso problem.\n",
      "\n",
      "#### Argument 2: geometric argument\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Combining L1 and L2 penalties: elastic nets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Fitting hyperparameters properly\n",
      "\n",
      "- Compared to OLS, results are now no longer invariant to scaling inputs\n",
      "- We need to set parameters (I've heard it argued that *setting hyperparameters is the last big open problem in statistics*)\n",
      "- So far, we hand waived this problem away"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conclusions\n",
      "\n",
      "- Linear regression is a solid trick\n",
      "- Lasso gives sparse solutions, Ridge does not\n",
      "- Elastic nets combines both penalties\n",
      "- Use `ElasticNetCV` with a wide range of inputs as your go to regression\n",
      "- [scikit-learn](http://scikit-learn.org/) is great"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.random.random((100, 5))\n",
      "alpha = np.arange(5)\n",
      "c = 12.\n",
      "\n",
      "y = np.dot(X,alpha) + c + np.random.normal(size=100)\n",
      "yclean = np.dot(X,alpha) + c\n",
      "\n",
      "plt.scatter(yclean, y)\n",
      "linreg.fit(X, y)\n",
      "print(\"Alpha is {}.\".format(linreg.coef_))\n",
      "print(\"Intercept is {}.\".format(linreg.intercept_))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is actually the *criterion of least squares*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame(boston.data, columns=boston.feature_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}