{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Penalized Linear Regression in Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Luis Pedro Coelho"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "<a href=\"http://luispedro.org\">luispedro.org</a><br />\n",
      "<img src=\"twitter32.png\" /><a href=\"http://twitter.com/luispedrocoelho\">@luispedrocoelho</a><br />\n",
      "<img src=\"github32.png\" /><a href=\"http://github.com/luispedro\">repositories on github</a><br />\n",
      "<a href=\"http://shop.oreilly.com/product/9781782161400.do\">Building Machine Learning Systems with Python: Willi Richert, Luis Pedro Coelho</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Notes on the presentation\n",
      "\n",
      "This presentation is an Ipython Notebook, which you can run at:\n",
      "\n",
      "LINK for presentation\n",
      "\n",
      "## Dependencies\n",
      "\n",
      "To run this, you need\n",
      "\n",
      "- ipython, version 2\n",
      "- numpy\n",
      "- matplotlib (for the plots)\n",
      "- scikit-learn\n",
      "- pandas\n",
      "\n",
      "Pandas is just a convenience"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Preliminary imports\n",
      "\n",
      "We will import `numpy` using the `np` abbreviation and `matplplotlib.pyplot` using the `plt` abbreviation:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython.display import HTML, display"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We also need to perform some magic to get plots inline:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Regression\n",
      "\n",
      "Regression can be used generically to mean \"any type of learning from data.\"\n",
      "\n",
      "More commonly, though, it is used to mean *learn to predict a **numeric** output from variables*. As opposed to **classification** which learns to predict a categorical output.\n",
      "\n",
      "### Examples\n",
      "\n",
      "- predicting prices\n",
      "- predicting blood sugar levels\n",
      "- predicting product ratings (collaborative filtering)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Example: Boston House prices\n",
      "\n",
      "The goal is to predict house prices in Boston based on variables such as\n",
      "\n",
      "1. number of rooms\n",
      "2. crime rate in area\n",
      "3. pupil teacher ratio\n",
      "4. ...\n",
      "\n",
      "\n",
      "### Loading data\n",
      "\n",
      "The `boston` dataset comes built in with scikit-learn:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_boston\n",
      "boston = load_boston()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can use `print(boston.DESCR)` to see more information on the dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Split into testing & training\n",
      "\n",
      "For all our analyses, it will be important to have split training & testing data.\n",
      "\n",
      "Scikit-learn makes this easy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split \n",
      "train_data, test_data, train_target, test_target = \\\n",
      "        train_test_split(boston.data, boston.target, train_size=.8)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- `train_data` is our training data with corresponding target variable `train_target`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Having a peak at the data\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is always a good idea to peak around, get a feeling for the data.\n",
      "\n",
      "Extremely important to look out for anomalies (real data is rarely clean)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(train_data.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can look at specific elements in the input data, such as the number of rooms:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RoomNr_Index = 5\n",
      "\n",
      "fig,ax = plt.subplots()\n",
      "ax.scatter(train_data[:,RoomNr_Index], train_target)\n",
      "ax.set_xlabel(\"Number of rooms\")\n",
      "ax.set_ylabel(\"House price\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Ordinary Least Squares Linear Regression\n",
      "\n",
      "Let $x$ be our inputs and $y$ our outputs.\n",
      "\n",
      "### Simple 1-D version\n",
      "\n",
      "Our *generative* model of the data is simply that given an input $x$, we obtain $y$ by\n",
      "\n",
      "1. multiply by $\\beta$,\n",
      "2. adding a constant ($c$)\n",
      "3. adding some noise ($\\epsilon$)\n",
      "\n",
      "$$y = \\beta x + c + \\epsilon$$\n",
      "\n",
      "### Multidimensional linear regression\n",
      "\n",
      "Instead of a single input variable, we can have multiple inputs, $x_1$, $x_2$, ... $x_n$:\n",
      "\n",
      "$$y = \\sum_j \\beta_j x_j + c + \\epsilon$$\n",
      "\n",
      "We can write the same using vector notation as:\n",
      "\n",
      "$$y = {\\boldsymbol \\beta}^T {\\bf x} + c + \\epsilon$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Least squares regression\n",
      "\n",
      "- In real life, we have observations and want to fit a model.\n",
      "- When we have more observations than datapoints, there is very little chance that a model will fit perfectly.\n",
      "- Therefore, we **minimise the fitting error**.\n",
      "\n",
      "$$y_i = {\\boldsymbol \\beta}^T {\\bf x}_i + c + \\epsilon_i$$\n",
      "\n",
      "We can measure the error as the sum of squared errors:\n",
      "\n",
      "$$ E = \\sum_i \\epsilon^2_i $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The criterion is thus\n",
      "\n",
      "$$ \\beta^{*} = \\arg\\min_{\\boldsymbol \\beta} \\sum_i ({\\boldsymbol \\beta}^T {\\bf x}_i + c - y_i)^2 $$\n",
      "\n",
      "which we can write in short form as:\n",
      "\n",
      "$$ \\beta^{*} = \\arg\\min_{\\boldsymbol \\beta} \\sum_i \\epsilon^2_i $$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### A few notes on the square function\n",
      "\n",
      "\n",
      "Pretty obvious but it'll come in handy later too:\n",
      "\n",
      "- larger error are penalized much more heavily than small error\n",
      "- doubling the error quadruples the penalty"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "E = np.linspace(0, 2, 100)\n",
      "squared_error_figure, ax = plt.subplots(figsize=(6,6))\n",
      "ax.plot(E, E**2)\n",
      "ax.plot([0,1], [1,1], 'k-')\n",
      "ax.plot([1,1], [0,1], 'k-')\n",
      "ax.set_xlim(0,4.)\n",
      "ax.set_xlabel('$E$')\n",
      "ax.set_ylabel('$E^2$')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "display(squared_error_figure)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Historical note\n",
      "\n",
      "This method of least squares was first developed to estimate the motion of cellular bodies by Gauss who was officially an astronomer. He also proved that least squares is optimal under *Gaussian* noise.\n",
      "\n",
      "![Gauss](http://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Carl_Friedrich_Gauss.jpg/330px-Carl_Friedrich_Gauss.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Using OLS on the boston data with scikit-learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "linreg = linear_model.LinearRegression()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1D version"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linreg.fit(train_data[:,RoomNr_Index:RoomNr_Index+1], train_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig,ax = plt.subplots()\n",
      "ax.scatter(train_data[:,RoomNr_Index], train_target)\n",
      "ax.plot([4, 9], linreg.predict([[4],[9]]), 'k-')\n",
      "ax.set_xlabel(\"Number of rooms\")\n",
      "ax.set_ylabel(\"House price\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multi-dimensional version\n",
      "\n",
      "We can use the same `linreg.fit` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linreg.fit(train_data, train_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Evaluating the results\n",
      "\n",
      "### Mean squared error\n",
      "\n",
      "This is a very natural way to measure the error:\n",
      "\n",
      "$$MSE = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
      "\n",
      "where $y_i$ is the actual value for example $i$, and $\\hat{y}_i$ is the prediction for the same example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "mse = metrics.mean_squared_error(test_target, linreg.predict(test_data)) \n",
      "print(\"MSE is {}\".format(mse))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "#### Problems\n",
      "\n",
      "Hard to interpret the value obtained.\n",
      "\n",
      "### Root mean squared error\n",
      "\n",
      "\n",
      "$$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}{N} (y_i - \\hat{y}_i)^2}$$\n",
      "\n",
      "1. Same units as original target variable\n",
      "2. If errors really are normal, then just double this value for an estimate of 95% confidence intervals."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rmse = np.sqrt(mse) \n",
      "print(\"RMSE is {}\".format(rmse))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Coefficient of determination\n",
      "\n",
      "Sometimes, there is another value that is very useful, the *coefficient of determination*:\n",
      "\n",
      "$$1 - \\frac{ \\sum_i (y_i -\\hat{y}_i)^2}{ \\sum_i  (y_i - \\bar{y})^2} \\approx 1 - \\frac{MSE}{VAR(y)}$$\n",
      "\n",
      "\n",
      "Where $\\bar{y}_i$ is the *average value of y$, i.e.:\n",
      "\n",
      "$$\\bar{y} = \\frac{\\sum_i y_i}{N}$$\n",
      "\n",
      "One way to think of this quantity is to consider the null predictor, which ignores the input and always returns a contant:\n",
      "\n",
      "    def predict(features):\n",
      "        return constant\n",
      "\n",
      "The *best constant is the mean*. The resulting value is 1 when the prediction is perfect, 0 when it is no better than always guessing the mean, and negative when it is worse"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Evaluating COD in Boston dataset\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cod = metrics.r2_score(test_target, linreg.predict(test_data))\n",
      "print(\"COD is {}\".format(cod))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This measure is also called $R^2$, which is confusing because it is not the square of anything.\n",
      "\n",
      "It is also called $Q^2$ in other literature.\n",
      "\n",
      "\n",
      "This measure is the default measure for regression in scikit-learn:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(linreg.score(test_data, test_target))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Training vs. Testing Data\n",
      "\n",
      "Naturally, the results on the training data are better than those obtained on the testing data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linreg.fit(train_data, train_target)\n",
      "r2_train = metrics.r2_score(train_target, linreg.predict(train_data))\n",
      "r2_test = metrics.r2_score(test_target, linreg.predict(test_data))\n",
      "\n",
      "print(\"R2 on training: {:.1%}\".format(r2_train))\n",
      "print(\"R2 on testing: {:.1%}\".format(r2_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will henceforth only focus on evaluting the results on testing data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Visualizing the fit\n",
      "\n",
      "We cannot easily see the "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linreg.fit(train_data, train_target)\n",
      "fig,ax = plt.subplots()\n",
      "ax.scatter(test_target, linreg.predict(test_data))\n",
      "ax.plot([0,50], [0,50], 'k-')\n",
      "ax.set_xlabel('Target (test)')\n",
      "ax.set_ylabel('Predicted (test)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Penalized (or Regularized) regression\n",
      "\n",
      "The criterion for least squared regression was:\n",
      "\n",
      "$$\\beta = \\arg\\min \\sum_j \\epsilon^2_j$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The general expresssion for penalized regression is\n",
      "\n",
      "$$\\beta = \\arg\\min \\sum_j \\epsilon^2_j + \\alpha R(\\beta),$$\n",
      "\n",
      "where $R$ is the penalty (regularization) term and $\\alpha$ is a positive weight."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### What about the intercept?\n",
      "\n",
      "- The intercept is typically not penalized.\n",
      "- One can simply pre-center the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### L1 and L2 penalties\n",
      "\n",
      "$L_1$ penalty means that we use the *sum of **absolute values***:\n",
      "\n",
      "$$P_1({\\boldsymbol \\beta}) = \\sum_j |\\beta_j|$$\n",
      "\n",
      "$L_2$ penalty means that we use the *sum of **squares***:\n",
      "\n",
      "$$P_2({\\boldsymbol \\beta}) = \\sum_j \\beta_j^2$$\n",
      "\n",
      "(The names come from the generic concept of an $L_p$ norm, see [Wikipedia](http://en.wikipedia.org/wiki/Lp_space) for details)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### L1 penalty: the lasso\n",
      "\n",
      "Using the L1 norm, leads us to the *Lasso*!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### L2 penalty: Ridge regression\n",
      "\n",
      "Using the L2 norm, leads us to *Ridge* regression (a.k.a. Tikhonov regularization).\n",
      "\n",
      "Unlike Lasso, which was invented only recently (1996), Ridge goes back a few decades."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Why does the Lasso return a sparse solution while Ridge does not?\n",
      "\n",
      "#### Argument 1: algorithmic argument\n",
      "\n",
      "Consider the following pseudo-algorithm:\n",
      "\n",
      "1. Start with ${\\boldsymbol \\beta} = 0$.\n",
      "2. Find the best index to increment $j$ and spend a bit of your budget to move $\\beta_j$ in the right direction.\n",
      "3. If this is better than before, then **goto 2**; else, stop.\n",
      "\n",
      "Because Ridge uses squared penalties, $\\beta$s that are still stuck at zero are very cheap in step 2, so we might choose them even if they are not very good.\n",
      "\n",
      "With Lasso, however, they always cost the same. Therefore, the algorithm will typically keep choosing the same index repeatedtly.\n",
      "\n",
      "*Note*: An improved version of this idea forms the basis of the famous [Least-Angle Regression](http://en.wikipedia.org/wiki/Least-angle_regression) method to solve the Lasso problem.\n",
      "\n",
      "#### Argument 2: geometric argument\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Combining L1 and L2 penalties: elastic nets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Fitting hyperparameters properly\n",
      "\n",
      "- Compared to OLS, results are now no longer invariant to scaling inputs\n",
      "- We need to set parameters (I've heard it argued that *setting hyperparameters is the last big open problem in statistics*)\n",
      "- So far, we hand waived this problem away"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conclusions\n",
      "\n",
      "- Linear regression is a solid trick\n",
      "- Lasso gives sparse solutions, Ridge does not\n",
      "- Elastic nets combines both penalties\n",
      "- Use `ElasticNetCV` with a wide range of inputs as your go to regression\n",
      "- [scikit-learn](http://scikit-learn.org/) is great"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.random.random((100, 5))\n",
      "alpha = np.arange(5)\n",
      "c = 12.\n",
      "\n",
      "y = np.dot(X,alpha) + c + np.random.normal(size=100)\n",
      "yclean = np.dot(X,alpha) + c\n",
      "\n",
      "plt.scatter(yclean, y)\n",
      "linreg.fit(X, y)\n",
      "print(\"Alpha is {}.\".format(linreg.coef_))\n",
      "print(\"Intercept is {}.\".format(linreg.intercept_))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is actually the *criterion of least squares*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame(boston.data, columns=boston.feature_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}